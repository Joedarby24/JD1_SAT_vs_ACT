{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1: SAT & ACT Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "\n",
    "The new format for the SAT was released in March 2016. As an employee of the College Board - the organization that administers the SAT - you are a part of a team that tracks statewide participation and recommends where money is best spent to improve SAT participation rates. Your presentation and report should be geared toward non-technical executives with the College Board and you will use the provided data and **outside research** to make recommendations about how the College Board might work to increase the participation rate in the state of **North Carolina**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executive Summary\n",
    "\n",
    "If you want to, it's great to use relative links to direct your audience to various sections of a notebook. **HERE'S A DEMONSTRATION WITH THE CURRENT SECTION HEADERS**:\n",
    "\n",
    "### Contents:\n",
    "- [2017 Data Import & Cleaning](#Data-Import-and-Cleaning)\n",
    "- [2018 Data Import and Cleaning](#2018-Data-Import-and-Cleaning)\n",
    "- [Exploratory Data Analysis](#Exploratory-Data-Analysis)\n",
    "- [Data Visualization](#Visualize-the-data)\n",
    "- [Descriptive and Inferential Statistics](#Descriptive-and-Inferential-Statistics)\n",
    "- [Outside Research](#Outside-Research)\n",
    "- [Conclusions and Recommendations](#Conclusions-and-Recommendations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If you combine your problem statement, executive summary, data dictionary, and conclusions/recommendations, you have an amazing README.md file that quickly aligns your audience to the contents of your project.** Don't forget to cite your data sources!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*All libraries used should be added here*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2017 Data Import and Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Read In SAT & ACT  Data\n",
    "\n",
    "Read in the `sat_2017.csv` and `act_2017.csv` files and assign them to appropriately named pandas dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code:\n",
    "df_act = pd.read_csv('C:/Users/joeda/desktop/general_assembly/dsi_master/02_week/project_1/project_1/data/act_2017.csv')\n",
    "df_sat = pd.read_csv('C:/Users/joeda/desktop/general_assembly/dsi_master/02_week/project_1/project_1/data/sat_2017.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Display Data\n",
    "\n",
    "Print the first 10 rows of each dataframe to your jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_act.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sat.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Verbally Describe Data\n",
    "\n",
    "Take your time looking through the data and thoroughly describe the data in the markdown cell below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** \n",
    "\n",
    "Both sets of data are broken down state by state, while the ACT dataframe has a row that compiles the national average. After analyzing `df_act.tail()` the ACT dataframe has more rows thant the SAT, and will need to be taken into account during data cleansing. Each dataframe has a column for a the percentage of graduates that participated in the associated test. From there, the dataframes provide a section by section scoring average with a total score average following. It is noteworthy that the scoring scale, as well as the testing category structures, are different between the two tests.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4a. Does the data look complete? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "Although the grading scales a subject structure of each test are distinct, the data seems to be complete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4b. Are there any obvious issues with the observations?\n",
    "\n",
    "**What is the minimum *possible* value for each test/subtest? What is the maximum *possible* value?**\n",
    "\n",
    "Consider comparing any questionable values to the sources of your data:\n",
    "- [SAT](https://blog.collegevine.com/here-are-the-average-sat-scores-by-state/)\n",
    "- [ACT](https://blog.prepscholar.com/act-scores-by-state-averages-highs-and-lows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_act['Science'].value_counts().sort_index().head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_act['Composite'].value_counts().sort_index().head(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sat['Math'].value_counts().sort_index().head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_act['Participation'].value_counts().tail(13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In the ACT dataframe, there is one very obvious outlier in the Science category with a score of 2.3. After refering to the source data article, this value should be 23.2.\n",
    "\n",
    "\n",
    "- Also in the ACT dataframe, there is one Composite value that could be a string with the value of 20.2xm making the series a object type. \n",
    "\n",
    "\n",
    "- Finally, in the Participation category in df_act, there is one value of 60% that seems to have a different formatting than the rest of the data in the series. This could make the series an object type rather than a float type.\n",
    "\n",
    "\n",
    "- As it relates to the SAT dataframe, the math section has a low outlier value of 52. After review of the source data article, this value should be 524. \n",
    "\n",
    "\n",
    "- Also in the SAT dataframe, the Participation category is an object type, rather than a float type.\n",
    "\n",
    "\n",
    "- Between both dataframes, there were some states where the participation rate in each test was <= 9%. While this is to be expected and was addressed by each data source article, the ACT only had one value of this type while the SAT had several. I am curious as to what state had an 8% participation rate in the ACT and why? It may simply relate to georgraphical preference, but it is possible that another reason could explain this event. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4c. Fix any errors you identified\n",
    "\n",
    "**The data is available** so there's no need to guess or calculate anything. If you didn't find any errors, continue to the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ACT Corrections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code\n",
    "df_act['Composite'] = df_act['Composite'].replace('20.2x', '20.2')\n",
    "df_act['Composite'].tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_act['Composite'] = df_act['Composite'].astype(dtype=float)\n",
    "df_act['Composite'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_act.iloc[21,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_act.iloc[21,5] = 23.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_act.iloc[21,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_act[df_act['Participation'] == ' 60%'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_act['Participation'][0] = \"60%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_act['Participation'] = df_act['Participation'].astype(str).str.strip('%').astype(float) / 100\n",
    "\n",
    "# or \n",
    "\n",
    "# change_part(df_act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_act.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAT Corrections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sat['Participation'] = df_sat['Participation'].astype(str).str.strip('%').astype(float) / 100\n",
    "\n",
    "# or \n",
    "\n",
    "# change_part(df_sat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sat[df_sat['Math'] == 52]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sat['Math'][20] = 524"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sat['Math'][[20]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. What are your data types? \n",
    "Display the data types of each feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_act.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sat.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What did you learn?\n",
    "- Do any of them seem odd?  \n",
    "- Which ones are not as they should be?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Fix Incorrect Data Types\n",
    "Based on what you discovered above, use appropriate methods to re-type incorrectly typed data.\n",
    "- Define a function that will allow you to convert participation rates to an appropriate numeric type. Use `map` or `apply` to change these columns in each dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code\n",
    "# def change_part(df):\n",
    "#     df['Participation'] = df['Participation'].astype(str).str.strip('%').astype(float) / 100\n",
    "    \n",
    "#     df['Participation'] = df['Participation']\n",
    "    \n",
    "#     return df['Participation']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Fix any individual values preventing other columns from being the appropriate type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sat.loc[20, 'Math']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code\n",
    "# df_sat['Math'][20] = 524\n",
    "# df_sat.loc[20, 'Math'] = 524\n",
    "\n",
    "# or \n",
    "\n",
    "# df_act.iloc[21,5] = 23.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Finish your data modifications by making sure the columns are now typed appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_act.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Display the data types again to confirm they are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code:\n",
    "df_act.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sat.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Rename Columns\n",
    "Change the names of the columns to more expressive names so that you can tell the difference the SAT columns and the ACT columns. Your solution should map all column names being changed at once (no repeated singular name-changes). **We will be combining these data with some of the data from 2018, and so you should name columns in an appropriate way**.\n",
    "\n",
    "**Guidelines**:\n",
    "- Column names should be all lowercase (you will thank yourself when you start pushing data to SQL later in the course)\n",
    "- Column names should not contain spaces (underscores will suffice--this allows for using the `df.column_name` method to access columns in addition to `df['column_name']`.\n",
    "- Column names should be unique and informative (the only feature that we actually share between dataframes is the state)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code\n",
    "sat_newc = {'State' : 'state',\n",
    "            'Participation' : 'participation_sat_17',\n",
    "            'Evidence-Based Reading and Writing': 'read_write_sect_sat_17',\n",
    "            'Math': 'math_sect_sat_17',\n",
    "            'Total': 'total_avg_sat_17'\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_newc = {'State' : 'state',\n",
    "            'Participation' : 'participation_act_17',\n",
    "            'English': 'eng_sect_act_17',\n",
    "            'Math': 'math_sect_act_17',\n",
    "            'Reading': 'read_sect_act_17',\n",
    "            'Science': 'sci_sect_act_17',\n",
    "            'Composite': 'total_avg_act_17'\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_act.rename(columns= act_newc, inplace=True)\n",
    "df_act.head()\n",
    "\n",
    "# researched https://cmdlinetips.com/2018/03/how-to-change-column-names-and-row-indexes-in-pandas/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sat.rename(columns= sat_newc, inplace=True)\n",
    "df_sat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Create a data dictionary\n",
    "\n",
    "Now that we've fixed our data, and given it appropriate names, let's create a [data dictionary](http://library.ucmerced.edu/node/10249). \n",
    "\n",
    "A data dictionary provides a quick overview of features/variables/columns, alongside data types and descriptions. The more descriptive you can be, the more useful this document is.\n",
    "\n",
    "Example of a Fictional Data Dictionary Entry: \n",
    "\n",
    "|Feature|Type|Dataset|Description|\n",
    "|---|---|---|---|\n",
    "|**county_pop**|*integer*|2010 census|The population of the county (units in thousands, where 2.5 represents 2500 people).| \n",
    "|**per_poverty**|*float*|2010 census|The percent of the county over the age of 18 living below the 200% of official US poverty rate (units percent to two decimal places 98.10 means 98.1%)|\n",
    "\n",
    "[Here's a quick link to a short guide for formatting markdown in Jupyter notebooks](https://jupyter-notebook.readthedocs.io/en/stable/examples/Notebook/Working%20With%20Markdown%20Cells.html).\n",
    "\n",
    "Provided is the skeleton for formatting a markdown table, with columns headers that will help you create a data dictionary to quickly summarize your data, as well as some examples. **This would be a great thing to copy and paste into your custom README for this project.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Feature|Type|Dataset|Description|\n",
    "|---|---|---|---|\n",
    "|**state (df_sat)**|object|SAT| The state that the corresponding data in the row describes.\n",
    "|**participation_sat_17**|float|SAT|The percentage of high-school graduates by state that take the test.\n",
    "|**read_write_sect_sat_17**|integer|SAT| The average score by state for the Reading and Writing section of the SAT.\n",
    "|**math_sect_sat_17**|integer|SAT|The average score by state for the Math section of the SAT.\n",
    "|**total_avg_sat_17**|integer|SAT|The average score by state for the all sections of the SAT.\n",
    "|**state (df_act)**|object|ACT|The state that the corresponding data in the row describes.\n",
    "|**participation_act_17**|float|ACT| The percentage of high-school graduates by state that take the test.\n",
    "|**eng_sect_act_17**|float|ACT|The average score by state for the English section of the ACT.\n",
    "|**math_sect_act_17**|float|ACT|The average score by state for the Math section of the ACT.\n",
    "|**read_sect_act_17**|float|ACT|The average score by state for the Reading section of the ACT.\n",
    "|**sci_sect_act_17**|float|ACT|The average score by state for the Science section of the ACT.\n",
    "|**total_avg_act_17**|float|ACT|The cumulative average score by state for the all sections of the ACT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Drop unnecessary rows\n",
    "\n",
    "One of our dataframes contains an extra row. Identify and remove this from the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code\n",
    "df_act.drop(df_act.index[0], axis=0).head()\n",
    "\n",
    "# referenced https://chrisalbon.com/python/data_wrangling/pandas_dropping_column_and_rows/ to learn how to drop by index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. Merge Dataframes\n",
    "\n",
    "Join the 2017 ACT and SAT dataframes using the state in each dataframe as the key. Assign this to a new variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code:\n",
    "df_merge = pd.merge(df_sat, df_act,on='state', validate='1:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11. Save your cleaned, merged dataframe\n",
    "\n",
    "Use a relative path to save out your data as `combined_2017.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code\n",
    "df_merge.to_csv('C:/Users/joeda/desktop/general_assembly/dsi_master/02_week/project_1/project_1/data/combined_2017.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2018 Data Import and Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Links to the 2018 ACT and SAT data are provided in the README. These data live in PDFs, and so you'll get to enjoy practicing some *manual* data collection. Save these data as a CSV in your `data` directory, and import, explore, and clean these data in the same way you did above. **Make sure you comment on your steps so it is clear *why* you are doing each process**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_s18 = pd.read_csv('C:/Users/joeda/desktop/general_assembly/dsi_master/02_week/project_1/project_1/data/SAT2018_Testing _Data - SAT 2018.csv')\n",
    "#import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_s18.head() #analyze headers and data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_s18['Participation'] = df_s18['Participation'].str.strip('%').astype(float) / 100 \n",
    "#changes percentage values to match that of other dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sat_newc_18 = {'State' : 'state',\n",
    "            'Participation' : 'participation_sat_18',\n",
    "            'Evidence-Based Reading and Writing': 'read_write_sect_sat_18',\n",
    "            'Math': 'math_sect_sat_18',\n",
    "            'Total': 'total_avg_sat_18'\n",
    "           }\n",
    "\n",
    "#Create dictionary to rename columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_s18.rename(columns= sat_newc_18, inplace=True)\n",
    "df_s18.head()\n",
    "#Pass through dictionary to rename columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_s18.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_a18 = pd.read_csv('C:/Users/joeda/desktop/general_assembly/dsi_master/02_week/project_1/project_1/data/SAT2018_Testing _Data - ACT 2018.csv')\n",
    "#import 2018 ACT csv data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_a18['Participation'] = df_a18['Participation'].str.strip('%').astype(float) / 100\n",
    "#changes percentage values to match that of other dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_a18.head() #Analyze headers and data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_newc_18 = {'State' : 'state',\n",
    "            'Participation' : 'participation_act_18',\n",
    "            'Composite': 'total_avg_act_18'\n",
    "           }\n",
    "#Create dictionary to rename columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_a18.rename(columns= act_newc_18, inplace=True)\n",
    "df_a18.head()\n",
    "#Pass through dictionary to rename columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_a18.iloc[8,0] = 'District of Columbia'\n",
    "df_a18.iloc[8,0]\n",
    "#Matching the caps case of the DC to match that of the other data frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_a18 = df_a18.drop(df_a18.index[20], axis=0)   #Maine was listed twice so I dropped the one of the duplicated rows\n",
    "df_a18['state'][18:22]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine your 2017 and 2018 data into a single dataframe\n",
    "Joining on state names should work, assuming you formatted all your state names identically. Make sure none of your columns (other than state) have identical names. Do yourself a favor and decide if you're encoding participation rates as floats or integers and standardize this across your datasets.\n",
    "\n",
    "Save the contents of this merged dataframe as `final.csv`.\n",
    "\n",
    "**Use this combined dataframe for the remainder of the project**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge_2 = pd.merge(df_merge, df_a18,on='state', validate='1:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge_3 = pd.merge(df_merge_2, df_s18,on='state', validate='1:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_merge_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.iloc[19] #Confirming a few records to make sure the merge was done properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.to_csv('C:/Users/joeda/desktop/general_assembly/dsi_master/02_week/project_1/project_1/data/final.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "\n",
    "\n",
    "### Summary Statistics\n",
    "Transpose the output of pandas `describe` method to create a quick overview of each numeric feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code:\n",
    "df_final.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manually calculate standard deviation\n",
    "\n",
    "$$\\sigma = \\sqrt{\\frac{1}{n}\\sum_{i=1}^n(x_i - \\mu)^2}$$\n",
    "\n",
    "- Write a function to calculate standard deviation using the formula above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code\n",
    "def calc_std(series):\n",
    "    series_sum = 0\n",
    "    i_val = []\n",
    "    series_sqdif = 0\n",
    "    \n",
    "    for num in series:\n",
    "        series_sum += num\n",
    "    \n",
    "    series_mean = series_sum / len(series)\n",
    "    \n",
    "    for i in series:\n",
    "        i = i - series_mean\n",
    "        i = i * i\n",
    "        i_val.append(i)\n",
    "    \n",
    "    series_sqdif= sum(i_val)\n",
    "    \n",
    "    std = round(np.sqrt(series_sqdif / len(series)),6)\n",
    "    \n",
    "    return std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_std(df_final['participation_act_17'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.std(df_final['participation_act_17'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use a **dictionary comprehension** to apply your standard deviation function to each numeric column in the dataframe.  **No loops**  \n",
    "- Assign the output to variable `sd` as a dictionary where: \n",
    "    - Each column name is now a key \n",
    "    - That standard deviation of the column is the value \n",
    "     \n",
    "*Example Output :* `{'ACT_Math': 120, 'ACT_Reading': 120, ...}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict = df_final.to_dict(orient='list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_dict['state']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_diction = {k:calc_std(v) for k,v in df_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_diction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do your manually calculated standard deviations match up with the output from pandas `describe`? What about numpy's `std` method?\n",
    "\n",
    "- **My function is most closely aligned with the np.std() method. There is a subtle, yet noticeable difference between my function and apd.describe().**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Investigate trends in the data\n",
    "Using sorting and/or masking (along with the `.head` method to not print our entire dataframe), consider the following questions:\n",
    "\n",
    "- Which states have the highest and lowest participation rates for the:\n",
    "    - 2017 SAT? - **High:** (DC, MI, CT, DE, NH)    |    **Low:** (ND, MS, IA, MO, UT)\n",
    "    - 2018 SAT? - **High:** (CO, CT, DE, MI, ID)    |    **Low:** (ND, SD, WY, NE, WI)\n",
    "    - 2017 ACT? - **High:** (MO, KY, WI, UT, TN)    |    **Low:** (ME, NH, RI, DE, PN)\n",
    "    - 2018 ACT? - **High:** (AL, KY, UT, WI, TN)    |    **Low:** (ME, NH, RI, DE, PN)\n",
    "- Which states have the highest and lowest mean total/composite scores for the:\n",
    "    - 2017 SAT? - **High:** (MN, WI, IA, MO, KS)    |    **Low:** (DC, DE, ID, MI, ME)\n",
    "    - 2018 SAT? - **High:** (MN, ME, ND, IA, KS)    |    **Low:** (DC, DE, WV, ID, UT)\n",
    "    - 2017 ACT? - **High:** (NH, MA, CT, ME, DC)    |    **Low:** (NV, MS, SC, HI, NC)\n",
    "    - 2018 ACT? - **High:** (CT, MA, NH, NY, MI)    |    **Low:** (NV, SC, MS, HI, AL)\n",
    "\n",
    "\n",
    "- Do any states with 100% participation on a given test have a rate change year-to-year?\n",
    "\n",
    "  ___ACT___\n",
    "     - **FELL** from 100%: **(CO, MO)**\n",
    "     - **GREW** to 100%: **(NE, OH)**\n",
    "    \n",
    "  ___SAT___\n",
    "     - **FELL** from 100%: **(DC)**\n",
    "     - **GREW** to 100%: **(CO, ID)**\n",
    "     \n",
    "     \n",
    "- Do any states show have >50% participation on *both* tests either year?\n",
    "    __2017__\n",
    "     - **(FL, GA, HI)**\n",
    "    __2018__\n",
    "     - **(FL, GA, HI, NC, SC)**\n",
    "     \n",
    "Based on what you've just observed, have you identified any states that you're especially interested in? **Make a note of these and state *why* you think they're interesting**.\n",
    "\n",
    "- **Colorado** interests me regarding its drastic change in participation rates from one test to the other. **Maine** interests me as they scored in the bottom 5 of states taking the SAT in 2017, but grew to a top 5 performer in 2018. What caused such a drastic swing in performance in one year? **Pensylvania** consistently ranks in the bottom 5 participating states for the ACT. I am curious if they have a relationship with the SAT or what other reasons causing them to rank amongst the lowest. Finally, **DC** interests me as their performance they were a top 5 participant in the SAT for 2017, but a top 5 performer in the ACT that year. In 2018, DC fell from a 100% participation rate in the SAT. I am curious if the overall better performance on the ACT in 2017 caused this.\n",
    "\n",
    "**You should comment on your findings at each step in a markdown cell below your code block**. Make sure you include at least one example of sorting your dataframe by a column, and one example of using boolean filtering (i.e., masking) to select a subset of the dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### SAT Participation\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# SAT 2017 Top 5 Participation\n",
    "\n",
    "df_sat.sort_values(by='participation_sat_17', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAT 2017 Bottom 5 Participation\n",
    "\n",
    "df_sat.sort_values(by='participation_sat_17', ascending=True).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAT 2018 Top 5 Participation\n",
    "\n",
    "df_s18.sort_values(by='participation_sat_18', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAT 2018 Bottom 5 Participation\n",
    "\n",
    "df_s18.sort_values(by='participation_sat_18', ascending=True).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### ACT Participation\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACT 2017 Top 5 Participation\n",
    "\n",
    "df_act.sort_values(by='participation_act_17', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACT 2017 Bottom 5 Participation\n",
    "\n",
    "df_act.sort_values(by='participation_act_17', ascending=True).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACT 2018 Top 5 Participation\n",
    "\n",
    "df_a18.sort_values(by='participation_act_18', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACT 2018 Bottom 5 Participation\n",
    "\n",
    "df_a18.sort_values(by='participation_act_18', ascending=True).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### SAT Scoring Performance\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAT 2017 Top 5 Average Score Total\n",
    "\n",
    "df_sat.sort_values(by='total_avg_sat_17', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAT 2017 Bottom 5 Average Score Total\n",
    "\n",
    "df_sat.sort_values(by='total_avg_sat_17', ascending=True).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAT 2018 Top 5 Average Score Total\n",
    "\n",
    "df_s18.sort_values(by='total_avg_sat_18', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAT 2018 Bottom 5 Average Score Total\n",
    "\n",
    "df_s18.sort_values(by='total_avg_sat_18', ascending=True).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### ACT Scoring Performance\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACT 2017 Bottom 5 Average Score Total\n",
    "\n",
    "df_act.sort_values(by='total_avg_act_17', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACT 2017 Bottom 5 Average Score Total\n",
    "\n",
    "df_act.sort_values(by='total_avg_act_17', ascending=True).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACT 2018 Top 5 Average Score Total\n",
    "\n",
    "df_a18.sort_values(by='total_avg_act_18', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACT 2018 Bottom 5 Average Score Total\n",
    "\n",
    "df_a18.sort_values(by='total_avg_act_18', ascending=True).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### States with a participation change rate\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ACT participation FELL from 100% from 2017 to 2018\n",
    "\n",
    "df_final[(df_final['participation_act_17'] == 1) & (df_final['participation_act_18'] < 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACT participation GREW to 100% from 2017 to 2018\n",
    "\n",
    "df_final[(df_final['participation_act_18'] == 1) & (df_final['participation_act_17'] < 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAT participation FELL from 100% from 2017 to 2018\n",
    "\n",
    "df_final[(df_final['participation_sat_17'] == 1) & (df_final['participation_sat_18'] < 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAT participation GREW to 100% from 2017 to 2018\n",
    "\n",
    "df_final[(df_final['participation_sat_18'] == 1) & (df_final['participation_sat_17'] < 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### States with >50% in both tests either year\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# States with more than 50% participation in both tests for 2017\n",
    "df_final[(df_final['participation_act_17'] > 0.5) & (df_final['participation_sat_17'] > 0.5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# States with more than 50% participation in both tests for 2018\n",
    "df_final[(df_final['participation_act_18'] > 0.5) & (df_final['participation_sat_18'] > 0.5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the data\n",
    "\n",
    "There's not a magic bullet recommendation for the right number of plots to understand a given dataset, but visualizing your data is *always* a good idea. Not only does it allow you to quickly convey your findings (even if you have a non-technical audience), it will often reveal trends in your data that escaped you when you were looking only at numbers.\n",
    "\n",
    "Some recommendations on plotting:\n",
    "- Plots have titles\n",
    "- Plots have axis labels\n",
    "- Plots have appropriate tick labels\n",
    "- All text is legible in a plot\n",
    "- Plots demonstrate meaningful and valid relationships\n",
    "- Plots are interpreted to aid understanding\n",
    "\n",
    "There is such a thing as too many plots, and there are a *lot* of bad plots. You might make some! (But hopefully not with the guided prompts below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use Seaborn's heatmap with pandas `.corr()` to visualize correlations between all numeric features\n",
    "\n",
    "Heatmaps are generally not appropriate for presentations, and should often be excluded from reports as they can be visually overwhelming. **However**, they can be extremely useful in identify relationships of potential interest (as well as identifying potential collinearity before modeling).\n",
    "\n",
    "*example*:\n",
    "```python\n",
    "sns.heatmap(df.corr())\n",
    "```\n",
    "\n",
    "Please take time to format your output, adding a title. Look through some of the additional arguments and options. (Axis labels aren't really necessary, as long as the title is informative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "mask = np.zeros_like(df_final.corr())\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "with sns.axes_style(\"white\"):\n",
    "    sns.set_style(\"ticks\", {\"xtick.major.size\": 8, \"ytick.major.size\": 8})\n",
    "    sns.heatmap(df_final.corr(), mask=mask, square=True, center=0, cmap='coolwarm', linewidth=.25, annot=True);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a custom function to subplot histograms\n",
    "\n",
    "We have data for two tests for two years. We only have composite (and not subtest scores) for the 2018 ACT. We should write a function that will take the names of 2+ columns and subplot histograms. While you can use pandas plotting or Seaborn here, matplotlib gives you greater control over all aspects of your plots.\n",
    "\n",
    "[Helpful Link for Plotting Multiple Figures](https://matplotlib.org/users/pyplot_tutorial.html#working-with-multiple-figures-and-axes)\n",
    "\n",
    "Here's some starter code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subplot_histograms(dataframe, list_of_columns, list_of_titles, list_of_xlabels):\n",
    "    nrows = int(np.ceil(len(list_of_columns)/2)) # Makes sure you have enough rows\n",
    "    fig, ax = plt.subplots(nrows=nrows, ncols=2, figsize = (12, 6)) # You'll want to specify your figsize\n",
    "    ax = ax.ravel() # Ravel turns a matrix into a vector, which is easier to iterate\n",
    "    \n",
    "    for i, column in enumerate(list_of_columns): # Gives us an index value to get into all our lists\n",
    "        ax[i].hist(dataframe[column]) # feel free to add more settings\n",
    "        # Set titles, labels, etc here for each subplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subplot_histograms(df_final[['total_avg_act_17', 'participation_sat_18']], ['total_avg_act_17', 'participation_sat_18'], ['total_avg_act_17', 'participation_sat_18'], ['total_avg_act_17', 'participation_sat_18'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot and interpret histograms \n",
    "For each of the following:\n",
    "- Participation rates for SAT & ACT\n",
    "- Math scores for SAT & ACT\n",
    "- Reading/verbal scores for SAT & ACT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subplot_histograms(dataframe, list_of_columns, list_of_titles, list_of_xlabels):\n",
    "    nrows = int(np.ceil(len(list_of_columns)/2)) # Makes sure you have enough rows\n",
    "    fig, ax = plt.subplots(nrows=nrows, ncols=2, figsize = (14, 12)) # You'll want to specify your figsize\n",
    "    ax = ax.ravel() # Ravel turns a matrix into a vector, which is easier to iterate\n",
    "    \n",
    "    for i, column in enumerate(list_of_columns): # Gives us an index value to get into all our lists\n",
    "        ax[i].set_title(list_of_titles[i])\n",
    "        ax[i].set_xlabel(list_of_xlabels[i])\n",
    "        ax[i].set_ylabel('Number of States')\n",
    "        ax[i].set_ylim(0,25)\n",
    "        ax[i].hist(dataframe[column])\n",
    "    plt.show()\n",
    "        # Set titles, labels, etc here for each subplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_list1 = ['participation_sat_17', 'participation_act_17', 'participation_sat_18', 'participation_act_18']\n",
    "ttl_list1 = ['SAT Participation % 2017', 'Act Participation % 2017', 'SAT Participation % 2018', 'Act Participation % 2018']\n",
    "xlab_list1 = ['% SAT Participation', '% ACT Participation','% SAT Participation', '% ACT Participation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "subplot_histograms(df_final, col_list1, ttl_list1, xlab_list1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subplot_histograms(dataframe, list_of_columns, list_of_titles, list_of_xlabels):\n",
    "    nrows = int(np.ceil(len(list_of_columns)/2)) # Makes sure you have enough rows\n",
    "    fig, ax = plt.subplots(nrows=nrows, ncols=2, figsize = (14, 12)) # You'll want to specify your figsize\n",
    "    ax = ax.ravel() # Ravel turns a matrix into a vector, which is easier to iterate\n",
    "    \n",
    "    for i, column in enumerate(list_of_columns): # Gives us an index value to get into all our lists\n",
    "        ax[i].set_title(list_of_titles[i])\n",
    "        ax[i].set_xlabel(list_of_xlabels[i])\n",
    "        ax[i].set_ylabel('Number of States')\n",
    "        ax[i].set_ylim(0,14)\n",
    "        ax[i].hist(dataframe[column])\n",
    "    plt.show()\n",
    "        # Set titles, labels, etc here for each subplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_list2 = ['math_sect_sat_17', 'math_sect_sat_18', 'math_sect_act_17']\n",
    "ttl_list2 = ['Average Math Score SAT 2017', 'Average Math Score SAT 2018', 'Average Math Score ACT 2017']\n",
    "xlab_list2 = ['SAT Math Score Distribution', 'SAT Math Score Distribution','ACT Math Score Distribution']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subplot_histograms(df_final, col_list2, ttl_list2, xlab_list2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final[df_final['state'] == 'North Carolina']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subplot_histograms(dataframe, list_of_columns, list_of_titles, list_of_xlabels):\n",
    "    nrows = int(np.ceil(len(list_of_columns)/2)) # Makes sure you have enough rows\n",
    "    fig, ax = plt.subplots(nrows=nrows, ncols=2, figsize = (14, 12)) # You'll want to specify your figsize\n",
    "    ax = ax.ravel() # Ravel turns a matrix into a vector, which is easier to iterate\n",
    "    \n",
    "    for i, column in enumerate(list_of_columns): # Gives us an index value to get into all our lists\n",
    "        ax[i].set_title(list_of_titles[i])\n",
    "        ax[i].set_xlabel(list_of_xlabels[i])\n",
    "        ax[i].set_ylabel('Number of States')\n",
    "        ax[i].hist(dataframe[column])\n",
    "    plt.show()\n",
    "        # Set titles, labels, etc here for each subplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_list3 = ['read_write_sect_sat_17', 'read_write_sect_sat_18', 'eng_sect_act_17', 'read_sect_act_17']\n",
    "ttl_list3 = ['Average Verbal Score SAT 2017', 'Average Verbal Score SAT 2018', 'Average English Score ACT 2017', 'Average Reading Score ACT 2017']\n",
    "xlab_list3 = ['SAT Verbal Score Distr.', 'SAT Verbal Score Distr.','ACT English Score Distr.', 'ACT Reading Score Distr.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subplot_histograms(df_final, col_list3, ttl_list3, xlab_list3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_list4 = ['total_avg_sat_17', 'total_avg_act_17', 'total_avg_sat_18', 'total_avg_act_18']\n",
    "ttl_list4 = ['Average Total Score SAT 2017', 'Average Total Score ACT 2017', 'Average Total Score SAT 2018', 'Average Total Score ACT 2018']\n",
    "xlab_list4 = ['SAT Total Score Distr.', 'ACT Total Score Distr.','SAT Total Distr.', 'ACT Total Score Distr.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subplot_histograms(df_final, col_list4, ttl_list4, xlab_list4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot and interpret scatter plots\n",
    "\n",
    "For each of the following:\n",
    "- SAT vs. ACT math scores for 2017\n",
    "- SAT vs. ACT verbal/reading scores for 2017\n",
    "- SAT vs. ACT total/composite scores for 2017\n",
    "- Total scores for SAT 2017 vs. 2018\n",
    "- Composite scores for ACT 2017 vs. 2018\n",
    "\n",
    "Plot the two variables against each other using matplotlib or Seaborn\n",
    "\n",
    "Your plots should show:\n",
    "- Two clearly labeled axes\n",
    "- A proper title\n",
    "- Using colors and symbols that are clear and unmistakable\n",
    "\n",
    "**Feel free to write a custom function, and subplot if you'd like.** Functions save both time and space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "\n",
    "plt.scatter(df_final['math_sect_sat_17'], df_final['math_sect_act_17'])\n",
    "\n",
    "plt.title(\"SAT vs ACT Math Scores 2017\")\n",
    "plt.ylabel(\"ACT Math Avg\", fontsize = 20)\n",
    "plt.xlabel(\"SAT Math Avg\", fontsize = 20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "\n",
    "plt.scatter(df_final['read_write_sect_sat_17'], df_final['read_sect_act_17'])\n",
    "\n",
    "plt.title(\"SAT vs ACT Verbal Scores 2017\")\n",
    "plt.ylabel(\"ACT Reading Avg\", fontsize = 20)\n",
    "plt.xlabel(\"SAT Verbal Avg\", fontsize = 20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "\n",
    "plt.scatter(df_final['total_avg_sat_17'], df_final['total_avg_act_17'])\n",
    "\n",
    "plt.title(\"SAT vs ACT Total Averages 2017\")\n",
    "plt.ylabel(\"ACT Avg\", fontsize = 20)\n",
    "plt.xlabel(\"SAT Avg\", fontsize = 20)\n",
    "plt.xlim(900,1400)\n",
    "plt.ylim(15,28);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "\n",
    "plt.scatter(df_final['total_avg_sat_18'], df_final['total_avg_act_18'])\n",
    "\n",
    "plt.title(\"SAT vs ACT Total Averages 2018\")\n",
    "plt.ylabel(\"ACT Avg\", fontsize = 20)\n",
    "plt.xlabel(\"SAT Avg\", fontsize = 20)\n",
    "plt.xlim(0,1600)\n",
    "plt.ylim(0,36);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot and interpret boxplots\n",
    "\n",
    "For each numeric variable in the dataframe create a boxplot using Seaborn. Boxplots demonstrate central tendency and spread in variables. In a certain sense, these are somewhat redundant with histograms, but you may be better able to identify clear outliers or differences in IQR, etc.\n",
    "\n",
    "Multiple values can be plotted to a single boxplot as long as they are of the same relative scale (meaning they have similar min/max values).\n",
    "\n",
    "Each boxplot should:\n",
    "- Only include variables of a similar scale\n",
    "- Have clear labels for each variable\n",
    "- Have appropriate titles and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subplot_boxplot(dataframe, list_of_columns, list_of_titles):\n",
    "    nrows = int(np.ceil(len(list_of_columns)/2)) # Makes sure you have enough rows\n",
    "    fig, ax = plt.subplots(nrows=nrows, ncols=2, figsize = (8, 20)) # You'll want to specify your figsize\n",
    "    ax = ax.ravel() # Ravel turns a matrix into a vector, which is easier to iterate\n",
    "    \n",
    "    for i, column in enumerate(list_of_columns): # Gives us an index value to get into all our lists\n",
    "        ax[i].set_title(list_of_titles[i])\n",
    "        ax[i].set_ylabel(\"% or Score\")\n",
    "        ax[i].boxplot(dataframe[column])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "box_1= list(df_final.columns)\n",
    "box_1 = box_1[1:5] + box_1[13:17] \n",
    "box_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subplot_boxplot(df_final, box_1, box_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "box_2= list(df_final.columns)\n",
    "box_2 = box_2[5:13] \n",
    "box_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subplot_boxplot(df_final, box_2, box_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feel free to do additional plots below\n",
    "*(do research and choose your own chart types & variables)*\n",
    "\n",
    "Are there any additional trends or relationships you haven't explored? Was there something interesting you saw that you'd like to dive further into? It's likely that there are a few more plots you might want to generate to support your narrative and recommendations that you are building toward. **As always, make sure you're interpreting your plots as you go**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (Optional): Using Tableau, create a choropleth map for each variable using a map of the US. \n",
    "\n",
    "Save this plot as an image file in an images directory, provide a relative path, and insert the image into notebook in markdown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descriptive and Inferential Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summarizing Distributions\n",
    "\n",
    "Above, we used pandas `describe` to provide quick summary statistics of our numeric columns. We also demonstrated many visual relationships.\n",
    "\n",
    "As data scientists, having a complete understanding of data is imperative prior to modeling.\n",
    "\n",
    "While we will continue to build our analytic tools, we know that measures of *central tendency*, *spread*, and *shape/skewness* provide a quick summary of distributions.\n",
    "\n",
    "For each variable in your data, summarize the underlying distributions (in words & statistics)\n",
    " - Be thorough in your verbal description of these distributions.\n",
    " - Be sure to back up these summaries with statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subplot_distplots(dataframe, list_of_columns, list_of_titles):\n",
    "    nrows = int(np.ceil(len(list_of_columns)/2)) # Makes sure you have enough rows\n",
    "    fig, ax = plt.subplots(nrows=nrows, ncols=2, figsize = (14, 18)) # You'll want to specify your figsize\n",
    "    ax = ax.ravel() # Ravel turns a matrix into a vector, which is easier to iterate\n",
    "    \n",
    "    for i, column in enumerate(list_of_columns): # Gives us an index value to get into all our lists\n",
    "        ax[i].set_title(list_of_titles[i])\n",
    "        ax[i].hist(dataframe[column])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subplot_distplots(df_final, box_1, box_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subplot_distplots(df_final, box_2, box_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We generally assuming that data we sample from a population will be normally distributed. Do we observe this trend?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answers: Across the board, we generally see a normal distribution as it relates to section scores and performance. They only area we see variation and skewing is in regards to state-by-state participation percentages. However, that is to be expected as that data is categorical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does This Assumption Hold for:\n",
    "    - Math\n",
    "    - Reading\n",
    "    - Rates\n",
    "Explain your answers for each distribution and how you think this will affect estimates made from these data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "    Yes, there is variation in the distributions of both the SAT math and reading sections from 2017 to 2018. However, the data still follows a normal distribution in a similar pattern as the previous yea along the scoring range, the independent variable axis.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimate Limits of Data\n",
    "\n",
    "Suppose we only seek to understand the relationship between SAT and ACT participation rates in 2017. \n",
    "\n",
    "##### Does it make sense to conduct statistical inference given these data specifically? \n",
    "\n",
    "Why or why not?\n",
    "\n",
    "*(think about granularity, aggregation, the relationships between populations size & rates...consider the actually populations these data describe in answering this question)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** Not especially. This specific type of data is categorical and varies widely from state to state. Generally speaking, test preference is geographically determined and it is safe to assume that if one test has a large participation percentage, then the other test has a much smaller participation percentage. \n",
    "\n",
    "States where you have a large percentage of students taking a particular test would be better samples to conduct statistical inference on, but only in regards to the given test for that state and not the national population. Nationally, there are several states with low or no participation percentage record for a given test, either the SAT or ACT. Also, there are many other local, georgraphic, and political considerations as well. When you account for these, as well as that wide variance in participation percentages across states, I do not believe that specific field should be used in inferential statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Is it appropriate to compare *these* specific SAT and ACT math scores? \n",
    "\n",
    "Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** I would not say that it is ideal. However, after looking at the distribution of the Math scores for both tests from 2017, I can see some striking similarities in the pattern of the data. You could compare Math scores from both tests in 2017 if there is a state that has an equal participation percentage in both tests for that year."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Statistical Evaluation of Distributions \n",
    "\n",
    "**If you feel it's appropriate**, using methods we discussed in class, run hypothesis tests to compare variables of interest in our dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outside Research"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based upon your observations, choose **three** states that demonstrate interesting trends in their SAT and/or ACT participation rates. Spend some time doing outside research on state policies that might influence these rates, and summarize your findings below. **Feel free to go back and create new plots that highlight these states of interest**. If you bring in any outside tables or charts, make sure you are explicit about having borrowed them. If you quote any text, make sure that it renders as being quoted. (Make sure that you cite your sources -- check with you local instructor for citation preferences)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_colorado= df_final[df_final['state'] == 'Colorado'][['state','participation_act_17', 'participation_act_18', 'participation_sat_17', 'participation_sat_18']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_colorado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_penn= df_final[df_final['state'] == 'Pennsylvania'][['state','participation_act_17', 'participation_act_18']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_penn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_idaho= df_final[df_final['state'] == 'Idaho'][['state','participation_act_17', 'participation_act_18', 'participation_sat_17', 'participation_sat_18']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_idaho"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions and Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on your exploration of the data, what are you key takeaways and recommendations? Choose one state with a lower participation rate and provide a suggestion for how the College Board might increase participation amongst graduating seniors in this state. Are there additional data you desire that would better inform your investigations?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "North Carolina:\n",
    "\n",
    "SAT participation grew slightly, from 49% to 52% in 2018\n",
    "2017 SAT Verbal Avg. = 68% | Math Avg. = ~67% \n",
    "2018 SAT Verbal Avg. = 69% | Math Avg. = ~68%  \n",
    "\n",
    "ACT participation was 100% in 2017 and 2018\n",
    "2017 ACT Verbal Avg. = 52% | Math Avg. = 53% \n",
    "2018 ACT Total Avg.=  53%\n",
    "\n",
    "First things first, we can compete! Althogh there is a public program where all 11th graders take the ACT for free, there is an opportunity to get footing in the market due to poor student performance on the ACT. We need to get the message to students that they perform better on the SAT in North Carolina. Encourage legislators, lobby groups, NC Board of Education members to fund all education and support math and reading development programs. Make a push, create educational programs, and offer discounts and promotional incentives to students, college boards, and the North Carolina Public School system to give more students access to the SAT.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project and Presentation References:\n",
    "### Colorado:\n",
    "- https://www.testive.com/colorado-sat-change-2017/\n",
    "- https://www.cde.state.co.us/assessment/coloradosat\n",
    "\n",
    "\n",
    "### Presentation:\n",
    "- http://www.ncpublicschools.org/accountability/act/\n",
    "- http://www.ncpublicschools.org/docs/accountability/policyoperations/1617actfaq.pdf\n",
    "- https://blog.collegevine.com/here-are-the-average-sat-scores-by-state/\n",
    "- https://blog.prepscholar.com/act-scores-by-state-averages-highs-and-lows\n",
    "- https://www.washingtonpost.com/news/grade-point/wp/2015/08/26/acts-college-admission-testing-grows-but-scores-stagnate/?utm_term=.dfb3efe5651c\n",
    "- https://www.washingtonpost.com/local/education/sat-to-drop-essay-requirement-and-return-to-top-score-of-1600-in-redesign-of-admission-test/2014/03/05/2aa9eee4-a46a-11e3-8466-d34c451760b9_story.html?noredirect=on&utm_term=.dc92be182d02\n",
    "- https://www.washingtonpost.com/education/2018/10/23/sat-reclaims-title-most-widely-used-college-admission-test/?utm_term=.be15e4c2f922\n",
    "- https://www.washingtonpost.com/local/education/sat-usage-declined-in-29-states-over-7-years/2014/03/15/f4504cfc-a5ff-11e3-8466-d34c451760b9_story.html?utm_term=.d9ba1080e4ed"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
